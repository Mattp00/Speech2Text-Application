{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/matteoparrotta/speech2text-model-training-and-evaluation?scriptVersionId=245813864\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction:\n\nThis part of the project aims to test and fine-tune some of the best open-source speech-to-text models for subsequent implementation in a client-server platform. The first part of the project involved a review of the literature on this specific case, the main paid services (using APIs), and the available open-source models. In this notebook, older and less performant models are not tested.\n\nNote: part of the finetuning code was extracted from the open-source guide available at: (https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb)","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install soundfile speechbrain accelerate \n!pip install evaluate jiwer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T09:09:43.959203Z","iopub.execute_input":"2025-06-16T09:09:43.959714Z","iopub.status.idle":"2025-06-16T09:11:05.976749Z","shell.execute_reply.started":"2025-06-16T09:09:43.95969Z","shell.execute_reply":"2025-06-16T09:11:05.975723Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nfrom datasets import load_dataset, DatasetDict, Audio\nfrom transformers import Wav2Vec2FeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport evaluate\nfrom torchmetrics.text import CharErrorRate\nfrom tqdm import tqdm\nimport torch\nimport time\nimport re\nfrom transformers.models.whisper.english_normalizer import BasicTextNormalizer\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\nHF_KEY = \"your_hf_key\"\nWHISPER_VERSION = \"openai/whisper-base\"\nWHISPER_LANGUAGE = \"it\"\nM4T2_LANGUAGE = \"ita\"\nwer_metric = evaluate.load(\"wer\")\ncer_metric = evaluate.load(\"cer\")\n\nlogin(token=HF_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T09:11:30.501886Z","iopub.execute_input":"2025-06-16T09:11:30.502191Z","iopub.status.idle":"2025-06-16T09:12:04.477119Z","shell.execute_reply.started":"2025-06-16T09:11:30.502163Z","shell.execute_reply":"2025-06-16T09:12:04.476583Z"}},"outputs":[{"name":"stderr","text":"2025-06-16 09:11:47.654299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750065107.840459      34 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750065107.896617      34 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f810c8947ec42a0b44340342e63a9c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e0f6aa8681e4c2492692d8b97fc19f7"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Section 1: Dataset implementation and pre-processing\n\nIt's important to state that the second part of code for each dataset are defined for the section 3 of the project. \n\n## 1.1) Dataset implementation","metadata":{}},{"cell_type":"code","source":"sampling_rate = 16000.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T09:20:07.755475Z","iopub.execute_input":"2025-06-16T09:20:07.755738Z","iopub.status.idle":"2025-06-16T09:20:07.759223Z","shell.execute_reply.started":"2025-06-16T09:20:07.755721Z","shell.execute_reply":"2025-06-16T09:20:07.758524Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### 1.1.1) DATASET 1: ITALIC","metadata":{}},{"cell_type":"code","source":"italic  = DatasetDict()\n\n#italic[\"train\"] = load_dataset(\"RiTA-nlp/ITALIC\",\"hard_speaker\", split=\"train+validation\", token=True,)\nitalic[\"test\"] = load_dataset(\"RiTA-nlp/ITALIC\",\"hard_speaker\",\n                              split=\"test\", token=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T09:00:18.136979Z","iopub.execute_input":"2025-06-16T09:00:18.13726Z","execution_failed":"2025-06-16T09:05:58.378Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/9.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e529981d2ec4be9b4ca76c65df3fff3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ITALIC.py:   0%|          | 0.00/9.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d51fa60b6844dacac25ed76d337f988"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/858 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61cbb523b4834966a7090e23c4bfb255"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for RiTA-nlp/ITALIC contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/RiTA-nlp/ITALIC.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.30G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"689a94e1abc14c6ca7915bbd88b0da7d"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"#Columns selection\ndataset = italic[\"test\"].select_columns([\"utt\",\"audio\"])\n\n#Set the sampling rate of the audio tracks\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n\ndataset = dataset.rename_column(\"utt\",\"text\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T20:38:27.139134Z","iopub.execute_input":"2025-06-15T20:38:27.139455Z","iopub.status.idle":"2025-06-15T20:38:27.148179Z","shell.execute_reply.started":"2025-06-15T20:38:27.139398Z","shell.execute_reply":"2025-06-15T20:38:27.147719Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### 1.1.2) DATASET 2: Common-Voice","metadata":{}},{"cell_type":"code","source":"common_voice = DatasetDict()\n\n\n#common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\",\"it\", split=\"train+validation\", token=True)\ncommon_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\",\"it\",\n                              split=\"test\", token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T09:19:00.462181Z","iopub.execute_input":"2025-06-16T09:19:00.462702Z","iopub.status.idle":"2025-06-16T09:19:57.733775Z","shell.execute_reply.started":"2025-06-16T09:19:00.462677Z","shell.execute_reply":"2025-06-16T09:19:57.73326Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2bc77f708c4efa84f016fa09b35ce7"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 15542it [00:00, 155406.81it/s]\u001b[A\nReading metadata...: 31564it [00:00, 158233.21it/s]\u001b[A\nReading metadata...: 47388it [00:00, 154165.18it/s]\u001b[A\nReading metadata...: 62817it [00:00, 151291.82it/s]\u001b[A\nReading metadata...: 78697it [00:00, 153933.90it/s]\u001b[A\nReading metadata...: 94104it [00:00, 152678.07it/s]\u001b[A\nReading metadata...: 109969it [00:00, 154597.09it/s]\u001b[A\nReading metadata...: 125438it [00:00, 154442.94it/s]\u001b[A\nReading metadata...: 140889it [00:00, 148912.18it/s]\u001b[A\nReading metadata...: 162637it [00:01, 150773.16it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0acf1f048b64380bd22299b1ee8ae5e"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 15086it [00:00, 178531.88it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bd3e53e375541a7935d6de6c5329fa6"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 15096it [00:00, 181452.24it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating other split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efd23fff393e4d4280076fce88366162"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 209it [00:00, 116896.86it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating invalidated split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ab2aa431da3495689b642e52b5829a9"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 17764it [00:00, 147264.26it/s]\u001b[A\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"dataset = common_voice[\"test\"].select_columns([\"audio\",\"sentence\"])\n\ndataset = dataset.rename_column(\"sentence\",\"text\")\n\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n\n#Selection of the first 1500 rows\ndataset = dataset.select(range(0,1500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T09:20:11.551619Z","iopub.execute_input":"2025-06-16T09:20:11.551893Z","iopub.status.idle":"2025-06-16T09:20:11.562741Z","shell.execute_reply.started":"2025-06-16T09:20:11.551873Z","shell.execute_reply":"2025-06-16T09:20:11.562188Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### 1.1.3) DATASET 3: Minds14","metadata":{}},{"cell_type":"code","source":"minds14 = DatasetDict()\n\n#minds14[\"train\"] = load_dataset(\"PolyAI/minds14\",\"it-IT\", split=\"train+validation\", token=True)\nminds14[\"test\"] = load_dataset(\"PolyAI/minds14\",\"it-IT\",\n                               token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T06:11:22.984144Z","iopub.execute_input":"2025-06-15T06:11:22.985105Z","iopub.status.idle":"2025-06-15T06:11:35.876125Z","shell.execute_reply.started":"2025-06-15T06:11:22.985073Z","shell.execute_reply":"2025-06-15T06:11:35.875607Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.28k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"124046cda56e47cd92b9034db813500a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"minds14.py:   0%|          | 0.00/5.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b881d67168bb49eb973f06de5b1dfe54"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for PolyAI/minds14 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/PolyAI/minds14.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"MInDS-14.zip:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47a60147690543a1a1a1e72df4cafc12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dbc61bc7f0040ce81ac4f114ca03fa6"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"dataset = minds14[\"test\"].select_columns([\"audio\",\"transcription\"])\n\ndataset = dataset.rename_column(\"transcription\",\"text\")\n\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n\n#Seleziono l'oggetto dataset\ndataset = dataset[\"train\"]\n\n#dataset = dataset.select(range(0,1500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T09:13:35.214673Z","iopub.status.idle":"2025-06-16T09:13:35.214894Z","shell.execute_reply.started":"2025-06-16T09:13:35.21479Z","shell.execute_reply":"2025-06-16T09:13:35.2148Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.1.4) DATASET 4: AMI (English Only)","metadata":{}},{"cell_type":"code","source":"ami = DatasetDict()\n\n#ami[\"train\"] = load_dataset(\"edinburghcstr/ami\",\"ihm\", split=\"train+validation\", token=True)\nami[\"test\"] = load_dataset(\"edinburghcstr/ami\",\"ihm\",\n                              split=\"test\", token=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = ami[\"test\"].select_columns([\"audio\",\"text\"])\n\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n\n#Select the first 1500 rowsr\ndataset = dataset.select(range(0,1500))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.2) Dataset preparation for whisper finetuning\n \n","metadata":{}},{"cell_type":"markdown","source":"### 1.2.1) Definition of whisper processor\n1) Whisper feature extractor\n2) Whisper tokenizer\n\nThis component enables the transformation of raw audio data to a suitable format for a subsequent analysis by the Whisper Model","metadata":{}},{"cell_type":"code","source":"#choose the preferred language\nprocessor = WhisperProcessor.from_pretrained(WHISPER_VERSION, language=WHISPER_LANGUAGE, task=\"transcribe\", device_map='auto')\n\nfeature_extractor = processor.feature_extractor\ntokenizer = processor.tokenizer\n\ninput_str = italic[\"train\"][0]['utt']\nlabels = tokenizer(input_str).input_ids\n\ndecoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\ndecoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n\n# CHECK if the processor code and decode correctly the labels (transcribed text)\nprint(f\"Input:                 {input_str}\")\nprint(f\"Decoded w/ special:    {decoded_with_special}\")\nprint(f\"Decoded w/out special: {decoded_str}\")\nprint(f\"Are equal:             {input_str == decoded_str}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:48:14.52229Z","iopub.execute_input":"2025-06-15T13:48:14.523022Z","iopub.status.idle":"2025-06-15T13:48:34.485638Z","shell.execute_reply.started":"2025-06-15T13:48:14.522989Z","shell.execute_reply":"2025-06-15T13:48:34.484836Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8adc56f387fe42ae80bf64ad9bb7dcf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d220852df6b24b31aa66f5a8abd70e40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc9ec59fa3874980a0f742817c5baa8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5208b0c5fc24a118e0f65c59ecf4139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a816912bd3c64d32873e4575e9c7bd5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34c3cbc8d4d64f45baec40b126ada235"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"048cc44434794cdbbc3fbc82da6422f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9286d22e8434487b8d2d5b73e8b1579"}},"metadata":{}},{"name":"stdout","text":"Input:                 svegliami alle cinque di mattina questa settimana\nDecoded w/ special:    <|startoftranscript|><|it|><|transcribe|><|notimestamps|>svegliami alle cinque di mattina questa settimana<|endoftext|>\nDecoded w/out special: svegliami alle cinque di mattina questa settimana\nAre equal:             True\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### 1.2.2) Definition of the Whisper Model (Encoder - Decoder) architecture","metadata":{}},{"cell_type":"code","source":"model = WhisperForConditionalGeneration.from_pretrained(WHISPER_VERSION,device_map='auto')\n\n#It's possible to choose the preferred language\nmodel.generation_config.language = WHISPER_LANGUAGE\nmodel.generation_config.task = \"transcribe\"\n\nmodel.generation_config.forced_decoder_ids = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:48:34.486855Z","iopub.execute_input":"2025-06-15T13:48:34.487133Z","iopub.status.idle":"2025-06-15T13:48:37.230947Z","shell.execute_reply.started":"2025-06-15T13:48:34.487104Z","shell.execute_reply":"2025-06-15T13:48:37.230189Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f190ada909a4501986ecfa7ef6c3f95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b66131e7dc7f4fa69f6477356f828257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c00078a81a4f40bff3157c214f0c5a"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"#The dataset preparation method\ndef prepare_dataset(batch):\n    # load and resample audio data from 48 to 16kHz\n    audio = batch[\"audio\"]\n\n    # compute log-Mel input features from input audio array \n    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n\n    # encode target text to label ids \n    batch[\"labels\"] = tokenizer(batch[\"utt\"]).input_ids\n    return batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:48:47.53367Z","iopub.execute_input":"2025-06-15T13:48:47.534386Z","iopub.status.idle":"2025-06-15T13:48:47.538518Z","shell.execute_reply.started":"2025-06-15T13:48:47.534357Z","shell.execute_reply":"2025-06-15T13:48:47.537844Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"italic = italic.map(prepare_dataset, remove_columns=italic.column_names[\"train\"], num_proc=8)\nitalic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:48:49.786629Z","iopub.execute_input":"2025-06-15T13:48:49.787417Z","iopub.status.idle":"2025-06-15T13:53:48.921441Z","shell.execute_reply.started":"2025-06-15T13:48:49.787388Z","shell.execute_reply":"2025-06-15T13:53:48.920694Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=8):   0%|          | 0/15080 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"376918ceb86f42eb8825aae0248a2501"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=8):   0%|          | 0/1441 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78cb57731a514eb0acd6874fd5fc325e"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_features', 'labels'],\n        num_rows: 15080\n    })\n    test: Dataset({\n        features: ['input_features', 'labels'],\n        num_rows: 1441\n    })\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Section 2: Finetuning (optional)\n\nThe fine-tuning step requires higher-level hardware. For example, if you try to use the P100 GPU provided by Kaggle, each iteration (forward + backward) of a batch (8 elements) for the Whisper Medium model takes approximately 17 seconds to complete.","metadata":{}},{"cell_type":"markdown","source":"## 2.1) Data Collator definition\nThis element is necessary to:\n1) Handles variable-length sequencies of data audio\n2) Enable an efficient batch processing on GPU","metadata":{}},{"cell_type":"code","source":"import torch\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    decoder_start_token_id: int\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        \n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        \n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:54:01.468476Z","iopub.execute_input":"2025-06-15T13:54:01.469244Z","iopub.status.idle":"2025-06-15T13:54:01.477545Z","shell.execute_reply.started":"2025-06-15T13:54:01.469217Z","shell.execute_reply":"2025-06-15T13:54:01.476742Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n    processor = processor,\n    decoder_start_token_id=model.config.decoder_start_token_id,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T13:54:06.763068Z","iopub.execute_input":"2025-06-15T13:54:06.763922Z","iopub.status.idle":"2025-06-15T13:54:06.768301Z","shell.execute_reply.started":"2025-06-15T13:54:06.763884Z","shell.execute_reply":"2025-06-15T13:54:06.767512Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 2.2) Hugging face trainer definition and training process","metadata":{}},{"cell_type":"code","source":"def compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    # replace -100 with the pad_token_id\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n    # we do not want to group tokens when computing the metrics\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = 100 * wer_metric.compute(predictions = pred_str, references = label_str)\n    cer = 100 * cer_metric.compute(predictions = pred_str, references = label_str)\n\n    return {\"wer\": wer, \"cer\": cer}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T14:23:26.701561Z","iopub.execute_input":"2025-06-15T14:23:26.702401Z","iopub.status.idle":"2025-06-15T14:23:26.707158Z","shell.execute_reply.started":"2025-06-15T14:23:26.702373Z","shell.execute_reply":"2025-06-15T14:23:26.706377Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"model.config.use_cache = False\n\nBATCH_SIZE = 128\n\ntraining_args = Seq2SeqTrainingArguments(\n    dataloader_num_workers = 4,\n    output_dir=\"./whisper_base\", \n    per_device_train_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=2, \n    learning_rate=1e-5,\n    warmup_steps=500,\n    max_steps=700,\n    gradient_checkpointing=True,\n    fp16=True,\n    eval_strategy=\"steps\",\n    per_device_eval_batch_size=BATCH_SIZE,\n    predict_with_generate=True,\n    generation_max_length=225,\n    save_steps=40,\n    eval_steps=20,\n    logging_steps=25,\n    report_to= \"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    #push_to_hub=True,\n    #hub_model_id=\"matteoparrott/whisper_large_v3_it\"\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T14:22:52.081575Z","iopub.execute_input":"2025-06-15T14:22:52.082537Z","iopub.status.idle":"2025-06-15T14:22:55.497542Z","shell.execute_reply.started":"2025-06-15T14:22:52.082496Z","shell.execute_reply":"2025-06-15T14:22:55.496783Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52687fbef01549c487a503129c1f772a"}},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"def partition_dataset(dataset, num_partitions):\n    partition_size = len(dataset) // num_partitions\n    return [dataset.select(range(i * partition_size, (i + 1) * partition_size)) for i in range(num_partitions)]\n\nnum_partitions = 16  # Numero di partizioni\npartitions = partition_dataset(italic[\"train\"], num_partitions)\npartitions[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T14:23:30.454437Z","iopub.execute_input":"2025-06-15T14:23:30.454727Z","iopub.status.idle":"2025-06-15T14:23:30.484579Z","shell.execute_reply.started":"2025-06-15T14:23:30.454707Z","shell.execute_reply":"2025-06-15T14:23:30.484025Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_features', 'labels'],\n    num_rows: 942\n})"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=partitions[0],\n    eval_dataset=partitions[1],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T14:26:28.224325Z","iopub.execute_input":"2025-06-15T14:26:28.22513Z","iopub.status.idle":"2025-06-15T14:26:28.240805Z","shell.execute_reply.started":"2025-06-15T14:26:28.225099Z","shell.execute_reply":"2025-06-15T14:26:28.240163Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3614144293.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T14:26:31.323588Z","iopub.execute_input":"2025-06-15T14:26:31.323902Z","execution_failed":"2025-06-15T15:55:01.677Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='79' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 79/700 47:06 < 6:19:52, 0.03 it/s, Epoch 19.50/175]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Wer</th>\n      <th>Cer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>No log</td>\n      <td>1.349006</td>\n      <td>52.061582</td>\n      <td>18.387127</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.380000</td>\n      <td>1.160457</td>\n      <td>48.646257</td>\n      <td>17.427231</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.185400</td>\n      <td>0.924857</td>\n      <td>41.019289</td>\n      <td>13.861017</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 2.3) Evaluation","metadata":{}},{"cell_type":"code","source":"eval_result = trainer.evaluate()\ntrainer.save_metrics(\"train\",eval_result)\neval_result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Section 3: Comparison M4T2 model - Whisper\n\nIn this section there's a comparison between two of the best open-source speech to text models. The metrics considered for the best model are:\n1) WER (Word Error Rate)\n2) CER (Character Error Rate)\n3) Inference time\n","metadata":{}},{"cell_type":"markdown","source":"## 3.1) Implementation of M4T2 model and data pre-processing","metadata":{}},{"cell_type":"code","source":"from transformers import AutoProcessor, SeamlessM4Tv2Model\nimport torch\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprocessor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\", device_map = 'auto')\nmodel = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\", device_map = 'auto')\n\n#model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T09:20:36.345696Z","iopub.execute_input":"2025-06-16T09:20:36.34597Z","iopub.status.idle":"2025-06-16T09:22:24.982973Z","shell.execute_reply.started":"2025-06-16T09:20:36.34595Z","shell.execute_reply":"2025-06-16T09:22:24.981999Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/1.78k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd9ba5722347409ca1b1ce6746cbe68d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/19.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f15966fadb1d438398d3890a2c4b9d75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.17M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab5141c8c05b41d3987fb7718b68ee75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a19f7b5d4aa4e9a8e209c7facba596a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19dd55983cce480892666500dcfc707e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44951030d12242eca15d26bb848e814b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/211k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00f42923a40a41b49a7c44cba55032a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40b0201145d64404838a27392b7934ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3855182dfef24a579d19ed93b940fe76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"130ee27217924b528cb87fc70dd2e11e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34802523c4354b54924edf0bb0195546"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/9.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec7c8be48bb476a9f98f6306833aef8"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### 3.1.1) Simple inference with the model","metadata":{}},{"cell_type":"code","source":"audio_sample = dataset[\"audio\"][0]\nlabel = dataset[\"text\"][0]\n\naudio_inputs = processor(audios=audio_sample[\"array\"], return_tensors=\"pt\", sampling_rate=16000)\naudio_inputs.to(device)\n\noutput_tokens = model.generate(**audio_inputs, tgt_lang=\"ita\", generate_speech=False)\ntranslated_text_from_audio = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\nprint(\"The text transcribed by the model is: \",translated_text_from_audio)\nprint(\"The label text is: \", label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T09:28:24.921304Z","iopub.execute_input":"2025-06-16T09:28:24.921628Z","iopub.status.idle":"2025-06-16T09:28:41.307951Z","shell.execute_reply.started":"2025-06-16T09:28:24.921607Z","shell.execute_reply":"2025-06-16T09:28:41.307212Z"}},"outputs":[{"name":"stdout","text":"The text transcribed by the model is:  Il libro ha suscitato molte polemiche a causa dei suoi contenuti.\nThe label text is:  Il libro ha suscitato molte polemiche a causa dei suoi contenuti.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## 3.2) Implementation of Whisper model","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperProcessor\nfrom transformers import WhisperForConditionalGeneration\n\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\", language=WHISPER_LANGUAGE, task=\"transcribe\", device_map='auto')\n\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\",device_map='auto')\n\nmodel.generation_config.language = \"it\"\nmodel.generation_config.task = \"transcribe\"\n\nmodel.generation_config.forced_decoder_ids = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T08:09:05.806422Z","iopub.execute_input":"2025-06-16T08:09:05.806695Z","iopub.status.idle":"2025-06-16T08:09:24.399126Z","shell.execute_reply.started":"2025-06-16T08:09:05.806675Z","shell.execute_reply":"2025-06-16T08:09:24.398629Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"263ea8f2bffb4f9bb2a77a29adb42e41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b899d7cb5b64d038aec6a461f16a102"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9af756ea1a36464d9c4fa251a2f9461f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2089f99454604b31bf1e2b2e9b0c2714"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f31ae0f69784a95ae61d343be7b39c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcf2fd523b6344c4b6f8a7aa685a57a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd6ca7686b6457bb9aa1e75ecb83597"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71ef11f9bd0849fd9cc22a534bfcb086"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db034c373aa4a83a37ce5c4f5cf4093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad7dccf2767e48c8b2fa8319d5336d68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.90k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfe4b31d678049fc89fd8b82aea5c44b"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## 3.3) Comparison between Whisper and M4T2","metadata":{}},{"cell_type":"markdown","source":"### 3.3.1) SeamlessM4T2-Test loop","metadata":{}},{"cell_type":"code","source":"length = dataset.num_rows\n\nall_predictions = []\nall_references = dataset['text'][0:length]\n\nt=0\n#for each item in the dataset, transcribe and store results in all_predictions\nfor i in tqdm(range(0,length)):\n    input_speech = dataset[i]['audio']\n    t0= time.time()\n    \n    input_features = processor(audios = input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\")\n    input_features.to(device)\n    with torch.no_grad(): \n        output_tokens = model.generate(**input_features, tgt_lang=M4T2_LANGUAGE, generate_speech=False)\n    \n    transcription = processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)\n    \n    t+= (time.time() - t0)\n    all_predictions.append(transcription)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T09:29:32.71119Z","iopub.execute_input":"2025-06-16T09:29:32.711548Z","iopub.status.idle":"2025-06-16T09:49:11.078161Z","shell.execute_reply.started":"2025-06-16T09:29:32.711526Z","shell.execute_reply":"2025-06-16T09:49:11.077415Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1500/1500 [19:38<00:00,  1.27it/s]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### 3.3.2) Whisper test loop","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nlength = dataset.num_rows\n\nall_predictions = []\nall_references = dataset['text'][0:length]\n\nt=0\n\nfor i in tqdm(range(0,length)):\n    input_speech = dataset[i]['audio']\n    t0= time.time()\n    input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features.to(device)\n    predicted_ids = model.generate(input_features)#, forced_decoder_ids=forced_decoder_ids)\n    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n    t+= (time.time() - t0)\n    all_predictions.append(transcription[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T08:16:19.650558Z","iopub.execute_input":"2025-06-16T08:16:19.651236Z","iopub.status.idle":"2025-06-16T08:40:56.038938Z","shell.execute_reply.started":"2025-06-16T08:16:19.651211Z","shell.execute_reply":"2025-06-16T08:40:56.038304Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/1500 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n100%|██████████| 1500/1500 [24:36<00:00,  1.02it/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 3.4) Result visualization\n\nTo compute the WER and CER metrics, a normalization step is necessary to remove elements such as extra whitespaces, uppercase letters, punctuation, and other artifacts. This step also handles Unicode normalization and diacritics removal, as performed by the Whisper normalizer.\n\nIt’s important to note that the WhisperBasicTextNormalizer performs well on English text, but its effectiveness in other languages may be limited.","metadata":{}},{"cell_type":"code","source":"import json\n\n#Metrics\nwer_metric = evaluate.load(\"wer\")\n\n#Normalizer provided by Whisper\nnormalizer = BasicTextNormalizer()\n\n#normalize both predictions and references. Also remove additional whitespace at the end of the predictions\nall_predictions_normalized = list(map(lambda x: normalizer(x), all_predictions))\nall_predictions_normalized = list(map(lambda x: re.sub(' $', '',x), all_predictions_normalized))\nall_references_normalized = list(map(lambda x: normalizer(x), all_references))\n\nwer = 100 * wer_metric.compute(\n    references=all_references_normalized, predictions=all_predictions_normalized\n)\n\n\ncer = 100 * cer_metric.compute(references = all_references_normalized, predictions = all_predictions_normalized )\n\n\nmetrics = {\n    'WER': wer,\n    'CER': cer,\n    'Time': t\n}\n\nprint('WERNorm:', wer)\nprint('CERNorm:', cer)\n\nwith open('metricsNorm(M4T2-ami).json', 'w') as f:\n    json.dump(metrics, f, indent=4)\n    \nwer = 100 * wer_metric.compute(\n    references=all_references, predictions=all_predictions\n)\ncer = 100 * cer_metric.compute(references = all_references, predictions = all_predictions )\n\nmetricsNorm = {\n    'WER': wer,\n    'CER': cer,\n    'Time': t\n}\n\nprint('WER:', wer)\nprint('CER:', cer)\nprint('Time:', t)\n\nwith open('metrics(M4T2-ami).json', 'w') as f:\n    json.dump(metricsNorm, f, indent=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T09:57:19.359085Z","iopub.execute_input":"2025-06-16T09:57:19.359355Z","iopub.status.idle":"2025-06-16T09:57:20.703279Z","shell.execute_reply.started":"2025-06-16T09:57:19.359337Z","shell.execute_reply":"2025-06-16T09:57:20.702659Z"}},"outputs":[{"name":"stdout","text":"WERNorm: 8.607060106310481\nCERNorm: 3.4840729679016733\nWER: 12.675958188153311\nCER: 4.381951822380261\nTime: 1161.9507732391357\n","output_type":"stream"}],"execution_count":21}]}