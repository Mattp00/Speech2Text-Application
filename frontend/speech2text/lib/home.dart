/* Unused imports 
import 'dart:async';
import 'dart:developer';
import 'dart:io';
import 'dart:js_interop';
import 'package:speech2text/function.dart';
import 'dart:html' as html;
*/

import 'dart:convert';

import 'dart:typed_data';
import 'dart:js' as js;

import 'package:audioplayers/audioplayers.dart';
import 'package:avatar_glow/avatar_glow.dart';
import 'package:flutter/material.dart';

import 'package:record/record.dart';
import 'package:http/http.dart' as http;

class Home extends StatefulWidget {
  const Home({Key? key}) : super(key: key);

  @override
  _HomeState createState() => _HomeState();
}

class _HomeState extends State<Home> {
  String url = '';
  var data;
  String output = '';
  bool _isListening = false;
  late AudioPlayer audioPlayer;
  late AudioRecorder audioRecord;
  final String path = "";
  var response;

  @override
  void dispose() {
    audioRecord.dispose();
    audioPlayer.dispose();
    super.dispose();
  }

  @override
  void initState() {
    audioPlayer = AudioPlayer();
    audioRecord = AudioRecorder();
    js.context.callMethod('initRecorder', []);
    super.initState();
  }

  bool _isRecording = false;
  Uint8List? _audioData;

  void _sendAudio() async {
    if (_audioData != null) {
      var url = Uri.parse('http://127.0.0.1:5000/upload_audio');
      print("Sending process to the server");
      var request = http.MultipartRequest('POST', url)
        ..files.add(http.MultipartFile.fromBytes('file', _audioData!,
            filename: 'recording.wav'));

      var streamedResponse = await request.send();
      var response = await http.Response.fromStream(streamedResponse);

      if (streamedResponse.statusCode == 200) {
        var decoded = jsonDecode(response.body);
        setState(() {
          output = decoded['output'];
          print('Uploaded successfully');
        });
      } else {
        print('Failed to upload');
      }
    }
  }

  void _startRecording() {
    js.context.callMethod('startRecording', []);
    setState(() {
      _isRecording = true;
    });
  }

  void _stopRecording() {
    js.context.callMethod('stopRecording', [
      js.JsFunction.withThis((self, audioData) {
        setState(() {
          _audioData = Uint8List.fromList(List<int>.from(audioData));
          _isRecording = false;
        });
      })
    ]);
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
          title: const Text('Frontend for Automatic Speech Recognition')),
      floatingActionButtonLocation: FloatingActionButtonLocation.centerFloat,
      floatingActionButton: AvatarGlow(
        animate: true,
        duration: const Duration(milliseconds: 2000),
        glowColor: Colors.blue,
        child: FloatingActionButton(
          onPressed: _isRecording ? _stopRecording : _startRecording,
          child: Icon(_isRecording ? Icons.mic : Icons.mic_none),
        ),
      ),
      body: Center(
        child: Container(
          padding: const EdgeInsets.all(20),
          child: Column(mainAxisAlignment: MainAxisAlignment.center, children: [
            if (output != "")
              const Text(
                "The transcription generated by the model is:",
                style: TextStyle(
                    fontSize: 40, color: Color.fromARGB(255, 76, 83, 175)),
              ),
            if (output != "")
              Text(
                output,
                style: const TextStyle(fontSize: 40, color: Colors.green),
              ),
            if (!_isListening)
              ElevatedButton(
                onPressed: _sendAudio,
                child: const Text("Send Audio"),
              ),
          ]),
        ),
      ),
    );
  }
}
